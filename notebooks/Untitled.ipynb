{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f0b0f87-3f01-4885-bc39-a4db06a712a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25e8af54-7d7f-495f-b996-6e0acae5b247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62d6f14b-6199-4b4d-8c39-756a30c70f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 128\n",
    "batch_size = 128\n",
    "num_epochs = 26\n",
    "lr = 0.005\n",
    "\n",
    "tokens = ['<', '>', '#', '%', ')', '(', '+', '-', '/', '.', '1', '0', '3', '2', '5', '4', '7',\n",
    "          '6', '9', '8', '=', 'A', '@', 'C', 'B', 'F', 'I', 'H', 'O', 'N', 'P', 'S', '[', ']',\n",
    "          '\\\\', 'c', 'e', 'i', 'l', 'o', 'n', 'p', 's', 'r', '\\n']\n",
    "tokens = ''.join(tokens) + ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab761732-3a8d-4ceb-997e-4853459d9510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity(input):\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f6ec337-1693-4b0e-b70f-4aa0de22e80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenChemMLP(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        self.params = params\n",
    "        self.hidden_size = self.params['hidden_size']\n",
    "        self.input_size = [self.params['input_size']] + self.hidden_size[:-1]\n",
    "        self.n_layers = self.params['n_layers']\n",
    "        self.activation = self.params['activation']\n",
    "        if type(self.activation) is list:\n",
    "            assert len(self.activation) == self.n_layers\n",
    "        else:\n",
    "            self.activation = [self.activation] * self.n_layers\n",
    "        if 'dropout' in self.params.keys():\n",
    "            self.dropout = self.params['dropout']\n",
    "        else:\n",
    "            self.dropout = 0\n",
    "        self.layers = nn.ModuleList([])\n",
    "        self.bn = nn.ModuleList([])\n",
    "        self.dropouts = nn.ModuleList([])\n",
    "        for i in range(self.n_layers - 1):\n",
    "            self.dropouts.append(nn.Dropout(self.dropout))\n",
    "            self.bn.append(nn.BatchNorm1d(self.hidden_size[i]))\n",
    "            self.layers.append(nn.Linear(in_features=self.input_size[i], out_features=self.hidden_size[i]))\n",
    "        i = self.n_layers - 1\n",
    "        self.dropouts.append(nn.Dropout(self.dropout))\n",
    "        self.layers.append(nn.Linear(in_features=self.input_size[i], out_features=self.hidden_size[i]))\n",
    "\n",
    "    @staticmethod\n",
    "    def get_required_params():\n",
    "        return {\n",
    "            'input_size': int,\n",
    "            'n_layers': int,\n",
    "            'hidden_size': list,\n",
    "            'activation': None,\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def get_optional_params():\n",
    "        return {'dropout': float}\n",
    "\n",
    "    def forward(self, inp):\n",
    "        output = inp\n",
    "        for i in range(self.n_layers - 1):\n",
    "            output = self.dropouts[i](output)\n",
    "            output = self.layers[i](output)\n",
    "            output = self.bn[i](output)\n",
    "            output = self.activation[i](output)\n",
    "        output = self.dropouts[-1](output)\n",
    "        output = self.layers[-1](output)\n",
    "        output = self.activation[-1](output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db724a6a-5e17-409e-b014-36a179b41434",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenChemEncoder(nn.Module):\n",
    "    \"\"\"Base class for embedding module\"\"\"\n",
    "    def __init__(self, params, use_cuda=None):\n",
    "        super(OpenChemEncoder, self).__init__()\n",
    "        self.params = params\n",
    "        if use_cuda is None:\n",
    "            use_cuda = torch.cuda.is_available()\n",
    "        self.use_cuda = use_cuda\n",
    "        self.input_size = self.params['input_size']\n",
    "        self.encoder_dim = self.params['encoder_dim']\n",
    "\n",
    "    @staticmethod\n",
    "    def get_required_params():\n",
    "        return {'input_size': int, 'encoder_dim': int}\n",
    "\n",
    "    @staticmethod\n",
    "    def get_optional_params():\n",
    "        return {}\n",
    "\n",
    "    def forward(self, inp):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class RNNEncoder(OpenChemEncoder):\n",
    "    def __init__(self, params, use_cuda):\n",
    "        super(RNNEncoder, self).__init__(params, use_cuda)\n",
    "        check_params(params, self.get_required_params(), self.get_optional_params())\n",
    "        self.layer = self.params['layer']\n",
    "        layers = ['LSTM', 'GRU', 'RNN']\n",
    "        if self.layer not in ['LSTM', 'GRU', 'RNN']:\n",
    "            raise ValueError(self.layer + ' is invalid value for argument'\n",
    "                             ' \\'layer\\'. Choose one from :' + ', '.join(layers))\n",
    "\n",
    "        self.input_size = self.params['input_size']\n",
    "        self.encoder_dim = self.params['encoder_dim']\n",
    "        self.n_layers = self.params['n_layers']\n",
    "        if self.n_layers > 1:\n",
    "            self.dropout = self.params['dropout']\n",
    "        else:\n",
    "            UserWarning('dropout can be non zero only when n_layers > 1. ' 'Parameter dropout set to 0.')\n",
    "            self.dropout = 0\n",
    "        self.bidirectional = self.params['is_bidirectional']\n",
    "        if self.bidirectional:\n",
    "            self.n_directions = 2\n",
    "        else:\n",
    "            self.n_directions = 1\n",
    "        if self.layer == 'LSTM':\n",
    "            self.rnn = nn.LSTM(self.input_size,\n",
    "                               self.encoder_dim,\n",
    "                               self.n_layers,\n",
    "                               bidirectional=self.bidirectional,\n",
    "                               dropout=self.dropout,\n",
    "                               batch_first=True)\n",
    "        elif self.layer == 'GRU':\n",
    "            self.rnn = nn.GRU(self.input_size,\n",
    "                              self.encoder_dim,\n",
    "                              self.n_layers,\n",
    "                              bidirectional=self.bidirectional,\n",
    "                              dropout=self.dropout,\n",
    "                              batch_first=True)\n",
    "        else:\n",
    "            self.layer = nn.RNN(self.input_size,\n",
    "                                self.encoder_dim,\n",
    "                                self.n_layers,\n",
    "                                bidirectional=self.bidirectional,\n",
    "                                dropout=self.dropout,\n",
    "                                batch_first=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_required_params():\n",
    "        return {\n",
    "            'input_size': int,\n",
    "            'encoder_dim': int,\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def get_optional_params():\n",
    "        return {'layer': str, 'n_layers': int, 'dropout': float, 'is_bidirectional': bool}\n",
    "\n",
    "    def forward(self, inp, previous_hidden=None, pack=True):\n",
    "        \"\"\"\n",
    "        inp: shape batch_size, seq_len, input_size\n",
    "        previous_hidden: if given shape n_layers * num_directions,\n",
    "        batch_size, embedding_dim.\n",
    "               Initialized automatically if None\n",
    "        return: embedded\n",
    "        \"\"\"\n",
    "        input_tensor = inp[0]\n",
    "        input_length = inp[1]\n",
    "        batch_size = input_tensor.size(0)\n",
    "        # TODO: warning: output shape is changed! (batch_first=True) Check hidden\n",
    "        if pack:\n",
    "            input_lengths_sorted, perm_idx = torch.sort(input_length, dim=0, descending=True)\n",
    "            input_lengths_sorted = input_lengths_sorted.detach().to(device=\"cpu\").tolist()\n",
    "            input_tensor = torch.index_select(input_tensor, 0, perm_idx)\n",
    "            rnn_input = pack_padded_sequence(input=input_tensor,\n",
    "                                             lengths=input_lengths_sorted,\n",
    "                                             batch_first=True)\n",
    "        else:\n",
    "            rnn_input = input_tensor\n",
    "        if previous_hidden is None:\n",
    "            previous_hidden = self.init_hidden(batch_size)\n",
    "            if self.layer == 'LSTM':\n",
    "                cell = self.init_cell(batch_size)\n",
    "                previous_hidden = (previous_hidden, cell)\n",
    "        else:\n",
    "            if self.layer == 'LSTM':\n",
    "                hidden = previous_hidden[0]\n",
    "                cell = previous_hidden[1]\n",
    "                hidden = torch.index_select(hidden, 1, perm_idx)\n",
    "                cell = torch.index_select(cell, 1, perm_idx)\n",
    "                previous_hidden = (hidden, cell)\n",
    "            else:\n",
    "                previous_hidden = torch.index_select(previous_hidden, 1, perm_idx)\n",
    "        rnn_output, next_hidden = self.rnn(rnn_input)  # , previous_hidden)\n",
    "\n",
    "        if pack:\n",
    "            rnn_output, _ = pad_packed_sequence(rnn_output, batch_first=True)\n",
    "            _, unperm_idx = perm_idx.sort(0)\n",
    "            rnn_output = torch.index_select(rnn_output, 0, unperm_idx)\n",
    "            if self.layer == 'LSTM':\n",
    "                hidden = next_hidden[0]\n",
    "                cell = next_hidden[1]\n",
    "                hidden = torch.index_select(hidden, 1, unperm_idx)\n",
    "                cell = torch.index_select(cell, 1, unperm_idx)\n",
    "                next_hidden = (hidden, cell)\n",
    "            else:\n",
    "                next_hidden = torch.index_select(next_hidden, 1, unperm_idx)\n",
    "\n",
    "        index_t = (input_length - 1).to(dtype=torch.long)\n",
    "        index_t = index_t.view(-1, 1, 1).expand(-1, 1, rnn_output.size(2))\n",
    "\n",
    "        embedded = torch.gather(rnn_output, dim=1, index=index_t).squeeze(1)\n",
    "\n",
    "        return embedded, next_hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        if self.use_cuda:\n",
    "            device = torch.device(\"cuda\")\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "        return torch.zeros(self.n_layers * self.n_directions, batch_size, self.encoder_dim, device=device)\n",
    "\n",
    "    def init_cell(self, batch_size):\n",
    "        if self.use_cuda:\n",
    "            device = torch.device(\"cuda\")\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "        return torch.zeros(self.n_layers * self.n_directions, batch_size, self.encoder_dim, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d023d3ee-77ae-4839-836a-53eb3068b955",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenChemEmbedding(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        self.params = params\n",
    "        self.num_embeddings = self.params['num_embeddings']\n",
    "        if 'padding_idx' in params.keys():\n",
    "            self.padding_idx = self.params['padding_idx']\n",
    "        else:\n",
    "            self.padding_idx = None\n",
    "\n",
    "    def forward(self, inp):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @staticmethod\n",
    "    def get_required_params():\n",
    "        return {\n",
    "            'num_embeddings': int,\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def get_optional_params():\n",
    "        return {'padding_idx': int}\n",
    "\n",
    "\n",
    "class Embedding(OpenChemEmbedding):\n",
    "    def __init__(self, params):\n",
    "        super().__init__(params)\n",
    "        self.embedding_dim = self.params['embedding_dim']\n",
    "        self.embedding = nn.Embedding(num_embeddings=self.num_embeddings,\n",
    "                                      embedding_dim=self.embedding_dim,\n",
    "                                      padding_idx=self.padding_idx)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        embedded = self.embedding(inp)\n",
    "        return embedded\n",
    "\n",
    "    @staticmethod\n",
    "    def get_required_params():\n",
    "        return {\n",
    "            'embedding_dim': int,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e4277b74-b3f3-4f7f-897e-5b6ecc3b5879",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'input_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [28], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m emb \u001b[38;5;241m=\u001b[39m Embedding({\n\u001b[1;32m      2\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mlen\u001b[39m(tokens),\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124membedding_dim\u001b[39m\u001b[38;5;124m'\u001b[39m: n_hidden,\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpadding_idx\u001b[39m\u001b[38;5;124m'\u001b[39m: tokens\u001b[38;5;241m.\u001b[39mindex(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m     })\n\u001b[0;32m----> 7\u001b[0m enc \u001b[38;5;241m=\u001b[39m \u001b[43mRNNEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_cuda\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_hidden\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlayer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLSTM\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mencoder_dim\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_hidden\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mn_layers\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdropout\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mis_bidirectional\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m mlp \u001b[38;5;241m=\u001b[39m OpenChemMLP({\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_size\u001b[39m\u001b[38;5;124m'\u001b[39m: n_hidden,\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_layers\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdropout\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     22\u001b[0m     })\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'input_size'"
     ]
    }
   ],
   "source": [
    "emb = Embedding({\n",
    "        'num_embeddings': len(tokens),\n",
    "        'embedding_dim': n_hidden,\n",
    "        'padding_idx': tokens.index(' ')\n",
    "    })\n",
    "\n",
    "enc = \n",
    "\n",
    "mlp = OpenChemMLP({\n",
    "        'input_size': n_hidden,\n",
    "        'n_layers': 2,\n",
    "        'hidden_size': [n_hidden, 1],\n",
    "        'activation': [F.relu, identity],\n",
    "        'dropout': 0.0\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9e3d08a-4f3d-4fac-91fd-9282405dc313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(\n",
       "  (embedding): Embedding(46, 128, padding_idx=45)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c64e43-c3a8-45f3-9e90-34fe98a54dd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
